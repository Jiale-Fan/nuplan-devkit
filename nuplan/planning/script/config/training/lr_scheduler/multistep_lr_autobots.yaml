_target_: torch.optim.lr_scheduler.MultiStepLR
_convert_: all

milestones: [2, 4, 6, 8, 10]  # decays the learning rate of each parameter group by gamma once the number of epochs equals one of the milestones
gamma: 0.5  # multiplicative factor of learning rate decay
